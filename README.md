# HCD Interview Coach

**Real-time AI support for UX research interviews**

A macOS application that captures system audio, transcribes conversations in real-time, and provides contextual coaching prompts to help researchers conduct better human-centered design interviews.

![macOS](https://img.shields.io/badge/macOS-13.0+-blue)
![Swift](https://img.shields.io/badge/Swift-5.9+-orange)
![License](https://img.shields.io/badge/license-MIT-green)

---

## Features

### Core Functionality
- **Real-time Transcription** â€” Live speech-to-text with speaker identification
- **Silence-First Coaching** â€” AI prompts that respect natural conversation flow
- **Topic Awareness** â€” Track coverage of interview guide topics
- **Insight Flagging** â€” Mark key moments manually (âŒ˜+I) or automatically
- **Session Export** â€” Export to Markdown or JSON for analysis
- **Built-in Interview Templates** â€” Discovery, Usability, Stakeholder, and Contextual templates
- **Liquid Glass UI** â€” Modern glassmorphism effects for a polished visual experience

### Design Philosophy
> "The best coaching is invisible until needed."

The app stays quiet by default, only surfacing prompts when:
- Confidence exceeds 85%
- 2+ minutes since last prompt
- 5+ seconds of silence after speech
- Fewer than 3 prompts shown this session

### Accessibility
- Full keyboard navigation
- VoiceOver support
- Color-independent indicators
- Reduced motion support
- WCAG 2.1 AA compliance

---

## Screenshots

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HCD Interview Coach                              âº 00:23:45 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚                                      â”‚
â”‚  ğŸ“‹ Topics           â”‚  Transcript                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  â— Onboarding   âœ“    â”‚  [00:21:32] Participant:             â”‚
â”‚  â— Pain Points       â”‚  "The biggest challenge is..."       â”‚
â”‚  â—‹ Workflows         â”‚                                      â”‚
â”‚  â—‹ Ideal State       â”‚  [00:22:15] Interviewer:             â”‚
â”‚                      â”‚  "Can you tell me more about that?"  â”‚
â”‚  ğŸ’¡ Insights (3)     â”‚                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚  [00:23:40] Participant:             â”‚
â”‚  â€¢ Onboarding gap    â”‚  "Well, when I first started..."     â”‚
â”‚  â€¢ Manual workaround â”‚                                      â”‚
â”‚  â€¢ Feature request   â”‚                                      â”‚
â”‚                      â”‚                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âŒ˜+R Start  âŒ˜+P Pause  âŒ˜+I Insight  âŒ˜+M Coaching  âŒ˜+S Export â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Requirements

- **macOS 13.0+** (Ventura or later)
- **Xcode 15.0+** (for development)
- **BlackHole 2ch** (virtual audio driver)
- **OpenAI API Key** (for transcription and coaching)

---

## Installation

### Quick Start

```bash
# Clone the repository
git clone https://github.com/dd-destrategy/HCD-buddy.git
cd HCD-buddy

# Install dependencies
brew install swiftlint blackhole-2ch

# Open in Xcode
open HCDInterviewCoach.xcodeproj

# Build and run (âŒ˜+R)
```

### Audio Setup

The app captures system audio via BlackHole virtual driver:

1. Install BlackHole: `brew install blackhole-2ch`
2. Open **Audio MIDI Setup** (Applications â†’ Utilities)
3. Click **+** â†’ **Create Multi-Output Device**
4. Check your speakers/headphones AND **BlackHole 2ch**
5. Set Multi-Output as system output in **System Settings â†’ Sound**

The app includes a setup wizard that guides you through this process.

### API Configuration

1. Get an API key from [OpenAI Platform](https://platform.openai.com/api-keys)
2. Launch the app
3. Go to **Settings** (âŒ˜+,) â†’ **API**
4. Enter your API key (stored securely in Keychain)

---

## Usage

### Starting a Session

1. **Select Mode**: Transcription Only or With Coaching
2. **Choose Template**: Select interview guide (optional)
3. **Configure Audio**: Verify Multi-Output device is active
4. **Start Recording**: Press âŒ˜+R or click Start

### During the Interview

| Action | Shortcut |
|--------|----------|
| Start/Stop | âŒ˜+R |
| Pause/Resume | âŒ˜+P |
| Flag Insight | âŒ˜+I |
| Toggle Speaker | âŒ˜+T |
| Search Transcript | âŒ˜+F |
| Toggle Coaching | âŒ˜+M |
| Export | âŒ˜+S |
| Settings | âŒ˜+, |

### After the Session

- Review AI-generated summary
- Edit flagged insights
- Add researcher notes
- Export to Markdown or JSON

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER                        â”‚
â”‚               SwiftUI Views + ViewModels                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     BUSINESS LAYER                           â”‚
â”‚    SessionManager â”‚ CoachingService â”‚ ExportService          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       DATA LAYER                             â”‚
â”‚   SwiftData â”‚ KeychainService â”‚ RealtimeAPIClient            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Purpose |
|-----------|---------|
| `SessionManager` | Orchestrates recording, transcription, and state |
| `CoachingService` | Implements silence-first coaching logic |
| `RealtimeAPIClient` | WebSocket connection to OpenAI |
| `AudioCaptureEngine` | System audio capture via BlackHole |

---

## Development

### Building

```bash
# Build from command line
xcodebuild -scheme HCDInterviewCoach -configuration Debug build

# Or in Xcode: âŒ˜+B
```

### Testing

```bash
# Run all tests
xcodebuild test -scheme HCDInterviewCoach -destination 'platform=macOS'

# Or in Xcode: âŒ˜+U
```

### Linting

```bash
# Check code style
swiftlint lint

# Auto-fix violations
swiftlint lint --fix
```

### Project Structure

```
HCD-buddy/
â”œâ”€â”€ HCDInterviewCoach/
â”‚   â”œâ”€â”€ Features/          # Feature modules
â”‚   â”‚   â”œâ”€â”€ AudioSetup/    # Setup wizard
â”‚   â”‚   â”œâ”€â”€ Coaching/      # Coaching engine
â”‚   â”‚   â”œâ”€â”€ Export/        # Export functionality
â”‚   â”‚   â”œâ”€â”€ Insights/      # Insight flagging
â”‚   â”‚   â”œâ”€â”€ PostSession/   # Summary view
â”‚   â”‚   â”œâ”€â”€ Session/       # Session management
â”‚   â”‚   â”œâ”€â”€ Topics/        # Topic tracking
â”‚   â”‚   â””â”€â”€ Transcript/    # Transcript display
â”‚   â”œâ”€â”€ Core/              # Models, services, utilities
â”‚   â””â”€â”€ DesignSystem/      # Design tokens and visual effects
â”‚       â”œâ”€â”€ Typography.swift
â”‚       â”œâ”€â”€ Spacing.swift
â”‚       â”œâ”€â”€ CornerRadius.swift
â”‚       â”œâ”€â”€ Shadows.swift
â”‚       â””â”€â”€ LiquidGlass.swift
â”œâ”€â”€ Tests/                 # Unit tests and mocks
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ .github/workflows/     # CI/CD pipelines
```

---

## Documentation

| Document | Description |
|----------|-------------|
| [SETUP.md](docs/SETUP.md) | Detailed local development setup |
| [CLAUDE.md](CLAUDE.md) | Project context for AI assistants |
| [CODEBASE_REVIEW.md](CODEBASE_REVIEW.md) | Architecture analysis |
| [APPROVED_DECISIONS.md](APPROVED_DECISIONS.md) | Key product decisions |
| [PRODUCT_BACKLOG.md](PRODUCT_BACKLOG.md) | Feature backlog |

---

## Tech Stack

| Category | Technology |
|----------|------------|
| Language | Swift 5.9+ |
| UI Framework | SwiftUI |
| Persistence | SwiftData |
| Audio | AVAudioEngine |
| AI/ML | OpenAI Realtime API |
| Security | macOS Keychain |
| CI/CD | GitHub Actions |
| Updates | Sparkle |
| Design System | Typography, Spacing, CornerRadius, Shadows, LiquidGlass |

---

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make changes with tests
4. Run linter (`swiftlint lint`)
5. Run tests (`âŒ˜+U`)
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Style

- Follow SwiftLint rules (`.swiftlint.yml`)
- Add `@MainActor` to `ObservableObject` classes
- Use `AppLogger` instead of `print()`
- Include accessibility labels on UI elements
- Write tests for new functionality

---

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- [OpenAI](https://openai.com) â€” Realtime API for transcription and AI
- [BlackHole](https://existential.audio/blackhole/) â€” Virtual audio driver
- [SwiftLint](https://github.com/realm/SwiftLint) â€” Code quality
- [Sparkle](https://sparkle-project.org) â€” Auto-updates

---

**Built with â¤ï¸ for UX researchers**
